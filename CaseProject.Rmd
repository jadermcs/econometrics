---
title: "Case Project"
author: "Jader Martins"
date: "01/05/2021"
output:
  pdf_document:
    fig_width: 5
    fig_height: 4
---

```{r setup, include=FALSE}
library(tidyverse)
data <- read.csv2("Case_HousePrices-round1.txt", sep = "\t")
knitr::opts_chunk$set(echo = TRUE)
```

## (a) Consider a linear model where the sale price of a house is the dependent variable and the explanatory variables are the other variables given above. Perform a test for linearity. What do you conclude based on the test result?

```{r}
houses.lm <- lm("sell ~ lot + bdms + fb + sty + drv + rec + ffin + ghw + ca + gar + reg", data)
summary(houses.lm)
```
We can check in the last column that most explanatory variables have high significance (***) and a p-value below 2.2e-16 so we reject the null hypothesis.

```{r fig1, fig.height=3, fig.align = "center", fig.cap="Histogram of regression residuals."}
houses.res <- residuals.lm(houses.lm)
hist(houses.res, xlab = "Residuals", main = "")
```

In the Figure $\ref{fig:fig1}$ we see that the residuals are approximately normally distributed with a heavy tail on the right side. So our estimation is not highly biased in any point of the price scale.

```{r fig2, fig.align = "center", fig.cap="Regression diagnosis plot."}
par(mfrow = c(2, 2))
plot(houses.lm)
```

In the Figure $\ref{fig:fig2}$ "Residual vs Fitted" plot we check an almost linear relation with no distinctive pattern, which is a good indication that our model capture the variation well. In the "Normal Q-Q" plot we check as above that out model is not biased in most of points and a little biased in extreme points of price. The "Scale-Location" plot is used to check if we have no heteroscedasticity so with an horizontal line and assuming that our points are ordered in time (no guarantee), we have a homoscedasticity indication. Finally, the "Residuals vs Leverage" indicates if extreme values in our regression are causing problems to the estimated line and with an horizontal line we have no problem with those points.

Concluding, as described above a linear regression is a good estimation for our dependent variable given out explanatory variables.

## (b) Now consider a linear model where the log of the sale price of the house is the dependent variable and the explanatory variables are as before. Perform again the test for linearity. What do you conclude now?

```{r}
houses.lm <- lm("log(sell) ~ lot + bdms + fb + sty + drv + rec + ffin + ghw + ca + gar + reg", data)
summary(houses.lm)
```

We've got a better adderence to the data with the log transformation.

```{r fig3, fig.height=3, fig.align = "center", fig.cap="Histogram of regression residuals."}
houses.res <- residuals.lm(houses.lm)
hist(houses.res, xlab = "Residuals", main = "")
```

In the Figure $\ref{fig:fig3}$ we can see that the heavy tail disappeared in the normal distribution.

```{r fig4, fig.align = "center", fig.cap="Regression diagnosis plot."}
par(mfrow = c(2, 2))
plot(houses.lm)
```

The problems from a biased estimation in extreme values also disappeared with the log transform, as shown in Figure $\ref{fig:fig4}$, so we can conclude that this transformation helps to estimate better our data.

## (c) Continue with the linear model from question (b). Estimate a model that includes both the lot size variable and its logarithm, as well as all other explanatory variables without transformation. What is your conclusion, should we include lot size itself or its logarithm?

```{r}
houses.lm <- lm("log(sell) ~ log(lot) + bdms + fb + sty + drv + rec + ffin + ghw + ca + gar + reg", data)
summary(houses.lm)
```

Include both variables, lot and log.lot, does not add significance to the model and as a result we got a worse F-test, so we conclude that is better to add only log.lot to the model.

## (d) Consider now a model where the log of the sale price of the house is the dependent variable and the explanatory variables are the log transformation of lot size, with all other explanatory variables as before. We now consider interaction effects of the log lot size with the other variables. Construct these interaction variables. How many are individually significant?

```{r}
houses.lm <- lm("log(lot) ~ log(sell) + bdms + fb + sty + drv + rec + ffin + ghw + ca + gar + reg", data)
summary(houses.lm)
```

The variables log.sell (transformed sale price), sty (number of stories excluding basement), drv (driveway in house), ffin (full finished basement), gar (number of covered garage places), are highly correlated with the log.lot which seems to be intuitively right, as they are related with the terrain size.

## (e) Perform an F-test for the joint significance of the interaction effects from question (d).
The F-test gave us 35.17 which is an small value, so many variables are uncorrelated with log.lot.

## (f) Now perform model specification on the interaction variables using the general-to-specific approach. (Only eliminate the interaction effects.)

```{r}
houses.lm <- lm("log(lot) ~ log(sell) + sty + drv + ffin + gar", data)
summary(houses.lm)
```

The value of F-test doubled so we have a much better model. But a lot of variability isn't explained yet with only those variables.

## (g) One may argue that some of the explanatory variables are endogenous and that there may be omitted variables. For example, the ‘condition’ of the house in terms of how it is maintained is not a variable (and difficult to measure) but will affect the house price. It will also affect, or be reflected in, some of the other variables, such as whether the house has an air conditioning (which is mostly in newer houses). If the condition of the house is missing, will the effect of air conditioning on the (log of the) sale price be over- or underestimated? (For this question no computer calculations are required.)

Without this proxy the prices will be underestimated as newer houses will not be accounted and they are, generally, more expensive.

## (f) Finally we analyze the predictive ability of the model. Consider again the model where the log of the sale price of the house is the dependent variable and the explanatory variables are the log transformation of lot size, with all other explanatory variables in their original form (and no interaction effects). Estimate the parameters of the model using the first 400 observations. Make predictions on the log of the price and calculate the MAE for the other 146 observations. How good is the predictive power of the model (relative to the variability in the log of the price)?

```{r}
houses.lm <- lm("log(sell) ~ lot + bdms + fb + sty + drv + rec + ffin + ghw + ca + gar + reg", data[1:400,])
houses.yhat <- predict(houses.lm, data[400:nrow(data),])
error <- houses.yhat - log(data[400:nrow(data),]$sell)
mae <- mean(abs(error))
mae
```

With a MAE of 0.1373367 we have a good estimation of house prices as it only misses exp(MAE) ~= 1, with a standard deviation of prices given by 22532.76.
